{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cogs118B-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0XaA79YqfAj"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('./preprocessed_train.csv', encoding='latin1')\n",
        "test_df = pd.read_csv('./preprocessed_test.csv', encoding='latin1')"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay2hM1DprSCu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86904501-3dcc-45c0-8d35-a3ff9896757e"
      },
      "source": [
        "import gzip\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import nltk\n",
        "import string\n",
        "from nltk.stem.porter import *\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "import scipy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7lIa6RtrVeb"
      },
      "source": [
        "dataset = train_df.values.tolist()\n",
        "test = test_df.values.tolist()"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx2DOlDWkUWA"
      },
      "source": [
        "dataset = [d[2:] for d in dataset if (type(d[2]) != float and type(d[3]) != float)]\n",
        "test = [d[2:] for d in test]\n",
        "train = dataset[:37040]\n",
        "validation = dataset[37040:]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhcV05gLC4xk",
        "outputId": "d08bd9c6-57d3-4289-abf1-e12508483c2a"
      },
      "source": [
        "train[0]"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Neutral',\n",
              " 'MeNyrbie PhilGahan Chrisitv httpstcoiFz9FAn2Pa and httpstcoxX6ghGFzCC and httpstcoI2NlzdxNo8']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "r3ukjuTl-PFH",
        "outputId": "002a25f6-594d-4be5-b35c-d8b4c86654a0"
      },
      "source": [
        "'''\n",
        "text = comp_df.corpus.values\n",
        "wordcloud = WordCloud(max_words=500,background_color='white', stopwords=stop_words, colormap='rainbow',height=300)\n",
        "wordcloud.generate(str(text))\n",
        "fig = plt.figure()\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(10)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ntext = comp_df.corpus.values\\nwordcloud = WordCloud(max_words=500,background_color='white', stopwords=stop_words, colormap='rainbow',height=300)\\nwordcloud.generate(str(text))\\nfig = plt.figure()\\nfig.set_figheight(6)\\nfig.set_figwidth(10)\\nplt.imshow(wordcloud)\\nplt.axis('off')\\nplt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtgDiSGCYuT"
      },
      "source": [
        "wordCount_base = defaultdict(int)\n",
        "punctuation = set(string.punctuation)\n",
        "numbers = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "for d in train:\n",
        "    r = ''.join([c for c in d[1].lower() if (c not in punctuation and c not in numbers)])\n",
        "    for w in r.split():\n",
        "        if w not in stop_words:\n",
        "            wordCount_base[w] += 1\n",
        "counts_base = [(wordCount_base[w], w) for w in wordCount_base]\n",
        "counts_base.sort()\n",
        "counts_base.reverse()\n",
        "words_base = [x[1] for x in counts_base[:4000]]\n",
        "wordId_base = dict(zip(words_base, range(len(words_base))))\n",
        "wordSet_base = set(words_base)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAlKMumvCemX"
      },
      "source": [
        "def feature_base(datum):\n",
        "    feat = [0]*len(words_base)\n",
        "    r = ''.join([c for c in datum[1].lower() if (c not in punctuation and c not in numbers)])\n",
        "    for w in r.split():\n",
        "        if w in words_base:\n",
        "            feat[wordId_base[w]] += 1\n",
        "    feat.append(1)\n",
        "    return feat"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50Tg3WgQCfoD"
      },
      "source": [
        "X_train_base = [feature_base(d) for d in train]\n",
        "y_train_base = [d[0] for d in train]\n",
        "X_validation_base = [feature_base(d) for d in validation]\n",
        "y_validation_base = [d[0] for d in validation]\n",
        "X_test_base = [feature_base(d) for d in test]\n",
        "y_test_base = [d[0] for d in test]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1-hywLyzTR3"
      },
      "source": [
        "#X_train_base[100:200]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "775Tb2RlCY-W"
      },
      "source": [
        "def change_sen(sentiment):\n",
        "    if sentiment == \"Extremely Positive\":\n",
        "        return 'positive'\n",
        "    elif sentiment == \"Extremely Negative\":\n",
        "        return 'negative'\n",
        "    elif sentiment == \"Positive\":\n",
        "        return 'positive'\n",
        "    elif sentiment == \"Negative\":\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'netural'\n",
        "y_train_base = [change_sen(s) for s in y_train_base]\n",
        "y_validation_base = [change_sen(s) for s in y_validation_base]\n",
        "y_test_base = [change_sen(s) for s in y_test_base]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW1D7AHdCkpj",
        "outputId": "ffbc7afe-c8a2-49e8-bb1e-b8c9ebf14e24"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "Xlil_base = scipy.sparse.lil_matrix(X_train_base)\n",
        "clf_base = LogisticRegression(C=1, solver='lbfgs', max_iter=10000, multi_class='auto')\n",
        "clf_base.fit(Xlil_base, y_train_base)\n",
        "prediction_base = clf_base.predict(X_test_base)\n",
        "acc_base = accuracy_score(prediction_base, y_test_base)\n",
        "print(f'Word counts model accuracy on test data is {acc_base}')\n",
        "print(classification_report(y_test_base, prediction_base))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word counts model accuracy on test data is 0.7835703001579779\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.82      0.78      0.80      1633\n",
            "     netural       0.66      0.68      0.67       619\n",
            "    positive       0.80      0.83      0.82      1546\n",
            "\n",
            "    accuracy                           0.78      3798\n",
            "   macro avg       0.76      0.76      0.76      3798\n",
            "weighted avg       0.78      0.78      0.78      3798\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "IJCEeFNBypIZ",
        "outputId": "f8f00d52-dae1-4a4d-b8dd-9280577e06c1"
      },
      "source": [
        "'''\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'C':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
        "lr = LogisticRegression(solver='lbfgs', max_iter=10000, multi_class='auto')\n",
        "clf = GridSearchCV(lr, parameters)\n",
        "clf.fit(Xlil_base, y_train_base)\n",
        "clf.cv_results_\n",
        "'''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfrom sklearn.model_selection import GridSearchCV\\nparameters = {'C':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\\nlr = LogisticRegression(solver='lbfgs', max_iter=10000, multi_class='auto')\\nclf = GridSearchCV(lr, parameters)\\nclf.fit(Xlil_base, y_train_base)\\nclf.cv_results_\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKshv62sl-GR",
        "outputId": "500f660a-ec7d-46ed-b90b-827de00c42c9"
      },
      "source": [
        "wordCounts3 = defaultdict(int)\n",
        "numbers = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "for d in train:\n",
        "    r = d[1].lower()\n",
        "    r = ''.join([c for c in r if (c not in punctuation and c not in numbers)])\n",
        "    ws = r.split()\n",
        "    ws = ws + [' '.join(x) for x in zip(ws[:-1], ws[1:])]    \n",
        "    for w in ws:\n",
        "      if w not in stop_words:\n",
        "        wordCounts3[w] += 1\n",
        "    \n",
        "\n",
        "print(len(wordCounts3))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "490365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AOGhcl2mfdh",
        "outputId": "a28c7c99-7953-4227-e3d5-cacece971aae"
      },
      "source": [
        "counts3 = [(wordCounts3[w], w) for w in wordCounts3]\n",
        "counts3.sort()\n",
        "counts3.reverse()\n",
        "\n",
        "counts3 = counts3\n",
        "\n",
        "words3 = [x[1] for x in counts3]\n",
        "\n",
        "wordId3 = dict(zip(words3, range(len(words3))))\n",
        "wordSet3 = set(words3)\n",
        "print(len(words3))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "490365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO3xloxDmlrb"
      },
      "source": [
        "def feature3(datum):\n",
        "    feat = [0]*len(words3)\n",
        "    r = ''.join([c for c in datum[1].lower() if (c not in punctuation and c not in numbers)])\n",
        "    ws = r.split()\n",
        "    ws = ws + [' '.join(x) for x in zip(ws[:-1], ws[1:])]\n",
        "    for w in ws:\n",
        "        if w in words3:\n",
        "            feat[wordId3[w]] += 1\n",
        "    feat.append(1) #offset\n",
        "    return feat"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "cuSl0ytimvJQ",
        "outputId": "b62c71b3-55da-4265-a379-b07ff72d3b0b"
      },
      "source": [
        "X_train3 = [feature3(d) for d in train]\n",
        "y_train3 = [d[0] for d in train]\n",
        "X_validation3 = [feature3(d) for d in validation]\n",
        "y_validation3 = [d[0] for d in validation]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-12573636090c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_train3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_validation3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_validation3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-12573636090c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_train3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_validation3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_validation3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-374a3f02f880>\u001b[0m in \u001b[0;36mfeature3\u001b[0;34m(datum)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mws\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mfeat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwordId3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#offset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPRWn2WknE8T"
      },
      "source": [
        "Xlil3 = scipy.sparse.lil_matrix(X_train3)\n",
        "clf = linear_model.LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=10000, C = 1)\n",
        "clf.fit(Xlil3, y_train3)\n",
        "pred = clf.predict(X_validation3)\n",
        "print(accuracy_score(pred, y_validation3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZuHeYGKpRYT"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "clf = SVC(kernel='linear', class_weight='balanced', C = 10)\n",
        "clf.fit(Xlil3, y_train3)\n",
        "prediction = clf.predict(X_validation3)\n",
        "print(accuracy_score(prediction, y_validation3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-TUrA334dJk"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(C=1, solver='lbfgs', max_iter=10000, multi_class='auto')\n",
        "clf.fit(Xlil3, y_train3)\n",
        "prediction = clf.predict(X_validation3)\n",
        "acc = accuracy_score(prediction, y_validation3)\n",
        "print(f'Baseline model accuracy on test data is {acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qpm3cMxHO75"
      },
      "source": [
        "'''\n",
        "from sklearn.svm import SVC\n",
        "clf = SVC(kernel='linear', class_weight='balanced', C = 10)\n",
        "clf.fit(Xlil_base, y_train_base)\n",
        "prediction = clf.predict(X_validation_base)\n",
        "print(accuracy_score(prediction, y_validation_base))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaz2M5WKD46K"
      },
      "source": [
        "TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km7BelHbkZBq"
      },
      "source": [
        "wordCount = defaultdict(int)\n",
        "punctuation = set(string.punctuation)\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "numbers = {'0','1','2','3','4','5','6','7','8','9'}\n",
        "for d in train:\n",
        "    r = ''.join([c for c in d[1].lower() if not (c in punctuation or c in numbers)])\n",
        "    d[1] = r\n",
        "    d[1] = tokenizer.tokenize(d[1])\n",
        "    d[1] = [token for token in d[1] if token not in stop_words]\n",
        "    for w in d[1]:\n",
        "        wordCount[w] += 1"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBi__44ykb0Q"
      },
      "source": [
        "counts = [(wordCount[w], w) for w in wordCount]\n",
        "counts.sort()\n",
        "counts.reverse()\n",
        "words_list = [x[1] for x in counts]\n",
        "words = set(words_list)\n",
        "for d in validation:\n",
        "    d[1] = tokenizer.tokenize(d[1])\n",
        "    d[1] = [token for token in d[1] if token not in stop_words]\n",
        "for d in test:\n",
        "  d[1] = tokenizer.tokenize(d[1])\n",
        "  d[1] = [token for token in d[1] if token not in stop_words]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zF9ROhPkesI"
      },
      "source": [
        "train_X = []\n",
        "train_y = []\n",
        "for d in train:\n",
        "    t = d[1]\n",
        "    train_X.append(' '.join(t))\n",
        "    train_y.append(d[0])\n",
        "validation_X = []\n",
        "validation_y = []\n",
        "for d in validation:\n",
        "    t = d[1]\n",
        "    tnew = []\n",
        "    for w in t:\n",
        "        if w in words:\n",
        "            tnew.append(w)\n",
        "    validation_X.append(' '.join(tnew))\n",
        "    validation_y.append(d[0])\n",
        "test_X = []\n",
        "test_y = []\n",
        "for d in test:\n",
        "    t = d[1]\n",
        "    tnew = []\n",
        "    for w in t:\n",
        "        if w in words:\n",
        "            tnew.append(w)\n",
        "    test_X.append(' '.join(tnew))\n",
        "    test_y.append(d[0])\n",
        "train_y = [change_sen(s) for s in train_y]\n",
        "validation_y = [change_sen(s) for s in validation_y]\n",
        "test_y = [change_sen(s) for s in test_y]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23BsS806RkQv",
        "outputId": "5c00704e-cab9-490c-f080-d42a59174e51"
      },
      "source": [
        "test[0]"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Extremely Negative',\n",
              " ['TRENDING',\n",
              "  'New',\n",
              "  'Yorkers',\n",
              "  'encounter',\n",
              "  'empty',\n",
              "  'supermarket',\n",
              "  'shelves',\n",
              "  'pictured',\n",
              "  'Wegmans',\n",
              "  'Brooklyn',\n",
              "  'soldout',\n",
              "  'online',\n",
              "  'grocers',\n",
              "  'FoodKick',\n",
              "  'MaxDelivery',\n",
              "  'coronavirusfearing',\n",
              "  'shoppers',\n",
              "  'stock',\n",
              "  'httpstcoGr76pcrLWh',\n",
              "  'httpstcoivMKMsqdT1']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX19-bEpkhUw",
        "outputId": "e91c23fc-4145-4d09-a087-39e6e1aed5f1"
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range = (1,2), max_features=4000, stop_words='english', max_df=0.5)\n",
        "vectorizer.fit(train_X)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=0.5, max_features=4000,\n",
              "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl5kP8Ymkliu"
      },
      "source": [
        "train_vector = vectorizer.transform(train_X)\n",
        "validation_vector = vectorizer.transform(validation_X)\n",
        "test_vector = vectorizer.transform(test_X)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zti7OVEGkmUx",
        "outputId": "f62ef7a9-ca82-4dde-a147-227d6e9ea0da"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(solver='saga', penalty='l1', multi_class='auto', max_iter=10000, C = 1)\n",
        "clf.fit(train_vector, train_y)\n",
        "print(clf.score(validation_vector, validation_y))\n",
        "prediction = clf.predict(test_vector)\n",
        "print(f'TF-IDF model accuracy on test set is {clf.score(test_vector, test_y)}')\n",
        "print(classification_report(test_y, prediction))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7419198055893074\n",
            "TF-IDF model accuracy on test set is 0.7345971563981043\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.74      0.76      1633\n",
            "     netural       0.55      0.69      0.61       619\n",
            "    positive       0.79      0.75      0.77      1546\n",
            "\n",
            "    accuracy                           0.73      3798\n",
            "   macro avg       0.70      0.72      0.71      3798\n",
            "weighted avg       0.74      0.73      0.74      3798\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyI6Uf_tRDAP",
        "outputId": "96e8f913-d357-424a-e7dd-864de6dd00cb"
      },
      "source": [
        "test_vector[5]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x6000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 0 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_qGnennAqu0"
      },
      "source": [
        "'''\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "lr = LogisticRegression(solver='lbfgs', max_iter=10000, multi_class='auto')\n",
        "clf = GridSearchCV(lr, parameters)\n",
        "clf.fit(train_vector, train_y)\n",
        "clf.cv_results_\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW4JN3Ovekj2"
      },
      "source": [
        "pdct = clf.predict(validation_vector)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C98fHwwHGQso",
        "outputId": "89fe937e-3951-4aa9-de21-3794c9a42248"
      },
      "source": [
        "print(pdct[:10])\n",
        "print(validation_y[:10])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['negative' 'netural' 'netural' 'netural' 'positive' 'positive' 'netural'\n",
            " 'positive' 'netural' 'positive']\n",
            "['negative', 'netural', 'positive', 'netural', 'positive', 'positive', 'netural', 'positive', 'netural', 'positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJhpZpLgGg1Q"
      },
      "source": [
        "print(validation[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVKkjfgx_4uv"
      },
      "source": [
        "'''\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf = KNeighborsClassifier(n_neighbors = 1)\n",
        "clf.fit(train_vector, train_y)\n",
        "prediction = clf.predict(validation_vector)\n",
        "print(accuracy_score(prediction, validation_y))\n",
        "print(prediction[:10])\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-LQdRjLKpCg",
        "outputId": "7592704a-1675-4093-f88c-cb474acfdd82"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "clf = SVC(kernel='linear', class_weight='balanced', C = 0.1)\n",
        "clf.fit(train_vector, train_y)\n",
        "prediction = clf.predict(validation_vector)\n",
        "print(accuracy_score(prediction, validation_y))\n",
        "print(prediction[:10])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6383961117861482\n",
            "['negative' 'netural' 'netural' 'netural' 'positive' 'positive' 'netural'\n",
            " 'positive' 'netural' 'positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qobabgchA5ro"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "svc = SVC(class_weight='balanced')\n",
        "clf = GridSearchCV(svc, parameters)\n",
        "clf.fit(train_vector, train_y)\n",
        "clf.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj6Qeq3Zl2Ld"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "clf = RandomForestClassifier(n_estimators = 500, random_state=0)\n",
        "clf.fit(train_vector, train_y)\n",
        "prediction = clf.predict(validation_vector)\n",
        "print(accuracy_score(prediction, validation_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S00UZU-RmeXA"
      },
      "source": [
        "print(prediction[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KJ5JWlemi2F"
      },
      "source": [
        "print(validation_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLnZGF-_monC"
      },
      "source": [
        "counts = [(wordCount[w], w) for w in wordCount]\n",
        "counts.sort()\n",
        "counts.reverse()\n",
        "words_list = [x[1] for x in counts[:2000]]\n",
        "words = set(words_list)\n",
        "for d in validation:\n",
        "    d[1] = tokenizer.tokenize(d[1])\n",
        "    d[1] = [token for token in d[1] if token not in stop_words]\n",
        "train_X = []\n",
        "train_y = []\n",
        "for d in train:\n",
        "    t = d[1]\n",
        "    train_X.append(' '.join(t))\n",
        "    train_y.append(d[0])\n",
        "validation_X = []\n",
        "validation_y = []\n",
        "for d in validation:\n",
        "    t = d[1]\n",
        "    tnew = []\n",
        "    for w in t:\n",
        "        if w in words:\n",
        "            tnew.append(w)\n",
        "    validation_X.append(' '.join(tnew))\n",
        "    validation_y.append(d[0])\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(train_X+validation_X)\n",
        "train_vector = vectorizer.transform(train_X)\n",
        "validation_vector = vectorizer.transform(validation_X)\n",
        "train_tfidf = pd.DataFrame(train_vector.todense(), columns = vectorizer.get_feature_names())\n",
        "validation_tfidf = pd.DataFrame(validation_vector.todense(), columns = vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLiijECnnJOc"
      },
      "source": [
        "clf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=10000, C = 1)\n",
        "clf.fit(train_tfidf, train_y)\n",
        "print(clf.score(validation_tfidf, validation_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXgdD1l2nUKH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}